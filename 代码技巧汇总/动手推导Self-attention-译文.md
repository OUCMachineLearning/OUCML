# åŠ¨æ‰‹æ¨å¯¼Self-attention

åœ¨ medium çœ‹åˆ°ä¸€ç¯‡æ–‡ç« ä»ä»£ç çš„è§’åº¦,ä½œè€…ç›´æ¥ç”¨ pytorch å¯è§†åŒ–äº† Attention çš„ QKV çŸ©é˜µ,ä¹‹å‰æˆ‘å¯¹ self-Attention çš„ç†è§£è¿˜æ˜¯æ¯”è¾ƒè¡¨é¢çš„,å¤§éƒ¨åˆ†æ—¶å€™ä¹Ÿæ˜¯ç›´æ¥å°±è°ƒç”¨ API æ¥ç”¨, çœ‹çœ‹åŸç†ä¹ŸæŒºæœ‰æ„æ€çš„,ä½œè€…åŒæ—¶åˆ¶ä½œäº†å¯åœ¨çº¿è¿è¡Œçš„ colabä½œä¸ºæ¼”ç¤º,é‚ç¿»è¯‘ç»™å¤§å®¶ä¸€èµ·çœ‹çœ‹:*The illustrations are best viewed on Desktop. A Colab version can be found* [*here*](https://colab.research.google.com/drive/1rPk3ohrmVclqhH7uQ7qys4oznDdAhpzF)*, (thanks to* [*Manuel Romero*](https://medium.com/u/3f2bb9b4510b?source=post_page-----2d627e33b20a----------------------)*!).*

[toc]

---

å‰è¨€:æœ‰äººé—®åœ¨` transformer `æ¨¡å‹çš„ä¼—å¤šæ´¾ç”ŸBERTï¼ŒRoBERTaï¼ŒALBERTï¼ŒSpanBERTï¼ŒDistilBERTï¼ŒSesameBERTï¼ŒSemBERTï¼ŒSciBERTï¼ŒBioBERTï¼ŒMobileBERTï¼ŒTinyBERTå’ŒCamemBERTæœ‰ä»€ä¹ˆå…±åŒç‚¹ï¼Ÿæˆ‘ä»¬çš„å¹¶ä¸æœŸå¾…ä½ å›ç­”éƒ½æœ‰å­—æ¯"BERT"ğŸ¤­.

äº‹å®ä¸Š,ç­”æ¡ˆæ˜¯ `Self-Attention`ğŸ¤—.æˆ‘ä»¬ä¸ä»…è¦è°ˆè®ºâ€œBERTâ€çš„æ¶æ„ï¼Œæ›´æ­£ç¡®åœ°è¯´æ˜¯åŸºäº``Transformer`æ¶æ„ã€‚åŸºäº`Transformer`çš„æ¶æ„ä¸»è¦ç”¨äºå¯¹è‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡è¿›è¡Œå»ºæ¨¡ï¼Œé¿å…ä½¿ç”¨ç¥ç»ç½‘ç»œä¸­çš„é€’å½’ç¥ç»ç½‘ç»œï¼Œè€Œæ˜¯å®Œå…¨ä¾èµ–`Self-Attention`æœºåˆ¶æ¥ç»˜åˆ¶è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„å…¨å±€ä¾å­˜å…³ç³»ã€‚ä½†æ˜¯ï¼Œè¿™èƒŒåçš„æ•°å­¦åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ

è¿™å°±æ˜¯æˆ‘ä»¬ä»Šå¤©è¦å‘æ˜çš„é—®é¢˜ã€‚è¿™ç¯‡æ–‡ç« çš„ä¸»è¦å†…å®¹æ˜¯å¼•å¯¼æ‚¨å®ŒæˆSelf-Attentionæ¨¡å—ä¸­æ¶‰åŠçš„æ•°å­¦è¿ç®—ã€‚åœ¨æœ¬æ–‡ç»“å°¾å¤„ï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿä»å¤´å¼€å§‹ç¼–å†™æˆ–ç¼–å†™`Self-Attention`æ¨¡å—ã€‚

æœ¬æ–‡çš„ç›®çš„å¹¶ä¸æ˜¯ä¸ºäº†é€šè¿‡æä¾›ä¸åŒçš„æ•°å­—è¡¨ç¤ºå½¢å¼å’Œæ•°å­¦è¿ç®—æ¥ç»™å‡º`Self-attention`çš„ç›´è§‚è§£é‡Šã€‚å®ƒä¹Ÿä¸æ˜¯ä¸ºäº†è¯æ˜:ä¸ºä»€ä¹ˆä¸”å¦‚ä½•åœ¨`Transformers `ä½¿ç”¨ä¸­`Self-Attention`ï¼ˆæˆ‘ç›¸ä¿¡é‚£é‡Œå·²ç»æœ‰å¾ˆå¤šä¸œè¥¿äº†ï¼‰ã€‚è¯·æ³¨æ„ï¼Œæœ¬æ–‡ä¹Ÿæ²¡æœ‰è¯¦ç»†ä»‹ç»æ³¨æ„åŠ›å’Œè‡ªæˆ‘æ³¨æ„åŠ›ä¹‹é—´çš„åŒºåˆ«ã€‚

## ä»€ä¹ˆæ˜¯è‡ªæ³¨æ„åŠ›æœºåˆ¶?

å¦‚æœä½ è®¤ä¸ºè‡ªæ³¨æ„åŠ›æœºåˆ¶ç±»ä¼¼äºæ³¨æ„åŠ›æœºåˆ¶,é‚£ä¹ˆæ­å–œä½ ç­”å¯¹äº†,å®ƒä»¬ä»æ ¹æœ¬ä¸Šæœ‰å¾ˆå¤šç›¸åŒçš„æ¦‚å¿µå’Œè®¸å¤šå¸¸è§çš„æ•°å­¦è¿ç®—ã€‚

ä¸€ä¸ª`self-attention`æ¨¡å—è¾“å…¥ä¸º n,è¾“å‡ºä¹Ÿä¸º n.é‚£ä¹ˆåœ¨è¿™ä¸ªæ¨¡å—å†…éƒ¨å‘ç”Ÿäº†ä»€ä¹ˆ?ç”¨é—¨å¤–æ±‰çš„æœ¯è¯­æ¥è¯´ï¼Œ`self-attention`æœºåˆ¶å…è®¸è¾“å…¥å½¼æ­¤ä¹‹é—´è¿›è¡Œäº¤äº’ï¼ˆâ€œselfâ€ï¼‰å¹¶æ‰¾å‡ºå®ƒä»¬åº”è¯¥æ›´å¤šå…³æ³¨çš„åŒºåŸŸï¼ˆâ€œAttentionâ€ï¼‰ã€‚è¾“å‡ºæ˜¯è¿™äº›äº¤äº’ä½œç”¨å’Œæ³¨æ„åŠ›å¾—åˆ†çš„æ€»å’Œã€‚

## å®ä¾‹æ¼”ç¤º

ä¾‹å­åˆ†ä¸ºä»¥ä¸‹æ­¥éª¤ï¼š

1. å‡†å¤‡è¾“å…¥
2. åˆå§‹åŒ–æƒé‡
3. å¯¼å‡º`key`, `query` and `value`çš„è¡¨ç¤º
4. è®¡ç®—è¾“å…¥1 çš„æ³¨æ„åŠ›å¾—åˆ†(`attention scores`)
5. è®¡ç®—softmax
6. å°†`attention scores`ä¹˜ä»¥`value`
7. å¯¹åŠ æƒåçš„`value`æ±‚å’Œä»¥å¾—åˆ°è¾“å‡º1
8. å¯¹è¾“å…¥2é‡å¤æ­¥éª¤4â€“7

> Note:
> å®é™…ä¸Šï¼Œæ•°å­¦è¿ç®—æ˜¯å‘é‡åŒ–çš„ï¼Œå³æ‰€æœ‰è¾“å…¥éƒ½ä¸€èµ·è¿›è¡Œæ•°å­¦è¿ç®—ã€‚æˆ‘ä»¬ç¨åä¼šåœ¨â€œä»£ç â€éƒ¨åˆ†ä¸­çœ‹åˆ°æ­¤ä¿¡æ¯ã€‚



1. å‡†å¤‡è¾“å…¥

    ![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/cy/2020-04-24-165651.png)

    Fig. 1.1: Prepare inputs

    åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä»3ä¸ªè¾“å…¥å¼€å§‹ï¼Œæ¯ä¸ªè¾“å…¥çš„å°ºå¯¸ä¸º4ã€‚

    ```
    Input 1: [1, 0, 1, 0] 
    Input 2: [0, 2, 0, 2]
    Input 3: [1, 1, 1, 1]
    ```

2. åˆå§‹åŒ–æƒé‡

    æ¯ä¸ªè¾“å…¥å¿…é¡»å…·æœ‰ä¸‰ä¸ªè¡¨ç¤ºå½¢å¼ï¼ˆè¯·å‚è§ä¸‹å›¾ï¼‰ã€‚è¿™äº›è¡¨ç¤ºç§°ä¸º`key`ï¼ˆæ©™è‰²ï¼‰ï¼Œ``query`ï¼ˆçº¢è‰²ï¼‰å’Œ`value`ï¼ˆç´«è‰²ï¼‰ã€‚åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œå‡è®¾æˆ‘ä»¬å¸Œæœ›è¿™äº›è¡¨ç¤ºçš„å°ºå¯¸ä¸º3ã€‚ç”±äºæ¯ä¸ªè¾“å…¥çš„å°ºå¯¸å‡ä¸º4ï¼Œè¿™æ„å‘³ç€æ¯ç»„æƒé‡çš„å½¢çŠ¶éƒ½å¿…é¡»ä¸º4Ã—3ã€‚

    > Note:
    > ç¨åæˆ‘ä»¬å°†çœ‹åˆ°`value`çš„ç»´åº¦ä¹Ÿå°±æ˜¯è¾“å‡ºçš„ç»´åº¦ã€‚

    ![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/cy/2020-04-24-165653.gif)

    Fig. 1.2: Deriving **key**, **query** and **value** representations from each input

    ä¸ºäº†è·å¾—è¿™äº›è¡¨ç¤ºï¼Œå°†æ¯ä¸ªè¾“å…¥ï¼ˆç»¿è‰²ï¼‰ä¹˜ä»¥ä¸€ç»„ç”¨äº`key`çš„æƒé‡ï¼Œå¦ä¸€ç»„ç”¨äº`query`çš„æƒé‡å’Œä¸€ç»„`value`çš„æƒé‡ã€‚åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å¦‚ä¸‹åˆå§‹åŒ–ä¸‰ç»„æƒé‡ã€‚

    `key`çš„æƒé‡

    ```
    [[0, 0, 1],
     [1, 1, 0],
     [0, 1, 0],
     [1, 1, 0]]
    ```

    `query`çš„æƒé‡

    ```
    [[1, 0, 1],
     [1, 0, 0],
     [0, 0, 1],
     [0, 1, 1]]
    ```

    `value`çš„æƒé‡

    ```
    [[0, 2, 0],
     [0, 3, 0],
     [1, 0, 3],
     [1, 1, 0]]
    ```

    > Note:
    >
    > åœ¨ç¥ç»ç½‘ç»œçš„è®¾ç½®ä¸­ï¼Œè¿™äº›æƒé‡é€šå¸¸æ˜¯å¾ˆå°çš„æ•°ï¼Œä½¿ç”¨é€‚å½“çš„éšæœºåˆ†å¸ƒï¼ˆå¦‚é«˜æ–¯ï¼ŒXavie å’Œ Kaiming åˆ†å¸ƒï¼‰éšæœºåˆå§‹åŒ–ã€‚åˆå§‹åŒ–åœ¨è®­ç»ƒä¹‹å‰å®Œæˆä¸€æ¬¡ã€‚*

3. ä»æ¯ä¸ªè¾“å…¥ä¸­å¯¼å‡º`key`, `query` and `value`çš„è¡¨ç¤º

    ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸‰ç»„å€¼çš„æƒé‡ï¼Œè®©æˆ‘ä»¬å®é™…æŸ¥çœ‹æ¯ä¸ªè¾“å…¥çš„**é”®**ï¼Œ**æŸ¥è¯¢**å’Œ**å€¼**è¡¨ç¤ºå½¢å¼ã€‚

    è¾“å…¥ 1 çš„`key`çš„è¡¨ç¤ºå½¢å¼

    ```
                   [0, 0, 1]
    [1, 0, 1, 0] x [1, 1, 0] = [0, 1, 1]
                   [0, 1, 0]
                   [1, 1, 0]
    ```

    ä½¿ç”¨ç›¸åŒçš„æƒé‡é›†è·å¾—è¾“å…¥ 2 çš„`key`çš„è¡¨ç¤ºå½¢å¼ï¼š

    ```
                   [0, 0, 1]
    [0, 2, 0, 2] x [1, 1, 0] = [4, 4, 0]
                   [0, 1, 0]
                   [1, 1, 0]
    ```

    ä½¿ç”¨ç›¸åŒçš„æƒé‡é›†è·å¾—è¾“å…¥ 3 çš„`key`çš„è¡¨ç¤ºå½¢å¼ï¼š

    ```
                   [0, 0, 1]
    [1, 1, 1, 1] x [1, 1, 0] = [2, 3, 1]
                   [0, 1, 0]
                   [1, 1, 0]
    ```

    ä¸€ç§æ›´å¿«çš„æ–¹æ³•æ˜¯å¯¹ä¸Šè¿°æ“ä½œè¿›è¡ŒçŸ©é˜µè¿ç®—ï¼š

    ```
                   [0, 0, 1]
    [1, 0, 1, 0]   [1, 1, 0]   [0, 1, 1]
    [0, 2, 0, 2] x [0, 1, 0] = [4, 4, 0]
    [1, 1, 1, 1]   [1, 1, 0]   [2, 3, 1]
    ```

    ![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/cy/2020-04-24-165656.gif)

    Fig. 1.3a: Derive **key** representations from each input

    è®©æˆ‘ä»¬åšåŒæ ·çš„äº‹æƒ…ä»¥è·å¾—æ¯ä¸ªè¾“å…¥çš„`value`è¡¨ç¤ºå½¢å¼ï¼š

    ```
                   [0, 2, 0]
    [1, 0, 1, 0]   [0, 3, 0]   [1, 2, 3] 
    [0, 2, 0, 2] x [1, 0, 3] = [2, 8, 0]
    [1, 1, 1, 1]   [1, 1, 0]   [2, 6, 3]
    ```

    ![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/cy/2020-04-24-165655.gif)

    Fig. 1.3b: Derive **value** representations from each input

    ä»¥åŠ`query`çš„è¡¨ç¤ºå½¢å¼:

    ```
                   [1, 0, 1]
    [1, 0, 1, 0]   [1, 0, 0]   [1, 0, 2]
    [0, 2, 0, 2] x [0, 0, 1] = [2, 2, 2]
    [1, 1, 1, 1]   [0, 1, 1]   [2, 1, 3]
    ```

    

    ![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/cy/2020-04-24-165654.gif)

    Fig. 1.3c: Derive **query** representations from each input

    > Notes:
    > å®é™…ä¸Šï¼Œå¯ä»¥å°†*åå·®å‘é‡* $b$ æ·»åŠ åˆ°çŸ©é˜µä¹˜æ³•çš„ä¹˜ç§¯ä¸­ã€‚
    >
    > (è¯‘è€…æ³¨:$y=w\cdot x+b $)

4. è®¡ç®—è¾“å…¥çš„æ³¨æ„åŠ›å¾—åˆ†(`attention scores`)

      ä¸ºäº†è·å¾—æ³¨æ„åŠ›åˆ†æ•°ï¼Œæˆ‘ä»¬é¦–å…ˆåœ¨è¾“å…¥1çš„`query`ï¼ˆçº¢è‰²ï¼‰ä¸æ‰€æœ‰`key`ï¼ˆæ©™è‰²ï¼‰ï¼ˆåŒ…æ‹¬å…¶è‡ªèº«ï¼‰ä¹‹é—´å–ç‚¹ç§¯ã€‚ç”±äºæœ‰3ä¸ª`key`è¡¨ç¤ºï¼ˆå› ä¸ºæˆ‘ä»¬æœ‰3ä¸ªè¾“å…¥ï¼‰ï¼Œå› æ­¤æˆ‘ä»¬è·å¾—3ä¸ªæ³¨æ„åŠ›å¾—åˆ†ï¼ˆè“è‰²ï¼‰ã€‚

    ```
                [0, 4, 2]
    [1, 0, 2] x [1, 4, 3] = [2, 4, 4]
                [1, 0, 1]
    ```

    ![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/cy/2020-04-24-170022.gif)

    Fig. 1.4: Calculating attention scores (blue) from query 1

    è¯·æ³¨æ„ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬ä»…ä½¿ç”¨è¾“å…¥1çš„`query`ã€‚ç¨åï¼Œæˆ‘ä»¬å°†å¯¹å…¶ä»–æŸ¥è¯¢é‡å¤ç›¸åŒçš„æ­¥éª¤ã€‚

    > Note:
    > ä¸Šé¢çš„æ“ä½œè¢«ç§°ä¸º"ç‚¹ç§¯æ³¨æ„åŠ›"ï¼Œæ˜¯å‡ ç§sorceä¹‹ä¸€ã€‚å…¶ä»–è¯„åˆ†åŠŸèƒ½åŒ…æ‹¬ç¼©æ”¾çš„ç‚¹ç§¯å’Œæ‹¼æ¥ã€‚
    >
    > æ›´å¤š sorce:https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3

5. è®¡ç®—softmax

    ![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/cy/2020-04-24-170124.gif)

    Fig. 1.5: Softmax the attention scores (blue)

    å°†`attention scores`é€šè¿‡ softmax å‡½æ•°(è“è‰²)å¾—åˆ°æ¦‚ç‡

    ```
    softmax([2, 4, 4]) = [0.0, 0.5, 0.5]
    ```

6. å°†`attention scores`ä¹˜ä»¥`value`

    ![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/cy/2020-04-24-170404.gif)

    Fig. 1.6: Derive **weighted** **value** representation (yellow) from multiply **value** (purple) and score (blue)

    æ¯ä¸ªè¾“å…¥çš„softmaxæ³¨æ„åŠ›å¾—åˆ†ï¼ˆè“è‰²ï¼‰ä¹˜ä»¥å…¶ç›¸åº”çš„`value`ï¼ˆç´«è‰²ï¼‰ã€‚è¿™å°†å¾—åˆ°3ä¸ªå¯¹é½çš„å‘é‡ï¼ˆé»„è‰²ï¼‰ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†å®ƒä»¬ç§°ä¸º"åŠ æƒå€¼"ã€‚

    ```
    1: 0.0 * [1, 2, 3] = [0.0, 0.0, 0.0]
    2: 0.5 * [2, 8, 0] = [1.0, 4.0, 0.0]
    3: 0.5 * [2, 6, 3] = [1.0, 3.0, 1.5]  
    ```

7. å¯¹åŠ æƒåçš„`value`æ±‚å’Œä»¥å¾—åˆ°è¾“å‡º1

      ![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/cy/2020-04-24-170619.gif)

    Fig. 1.7: Sum all **weighted values** (yellow) to get Output 1 (dark green)

    å¯¹æ‰€æœ‰`åŠ æƒå€¼`(é»„è‰²)æŒ‰å…ƒç´ æ±‚å’Œï¼š

    ```
      [0.0, 0.0, 0.0]
    + [1.0, 4.0, 0.0]
    + [1.0, 3.0, 1.5]
    -----------------
    = [2.0, 7.0, 1.5]
    ```

    å¾—åˆ°çš„å‘é‡[2.0, 7.0, 1.5] (æ·±ç»¿)æ˜¯è¾“å‡º 1 , å®ƒæ˜¯åŸºäºâ€œè¾“å…¥1â€çš„â€œ`query`è¡¨ç¤ºçš„å½¢å¼â€ ä¸æ‰€æœ‰å…¶ä»–`key`(åŒ…æ‹¬å…¶è‡ªèº«ï¼‰è¿›è¡Œçš„äº¤äº’ã€‚

8. å¯¹è¾“å…¥2é‡å¤æ­¥éª¤4â€“7

    ç°åœ¨æˆ‘ä»¬å·²ç»å®Œæˆäº†è¾“å‡º1ï¼Œæˆ‘ä»¬å°†å¯¹è¾“å‡º2å’Œè¾“å‡º3é‡å¤æ­¥éª¤4è‡³7ã€‚æˆ‘ç›¸ä¿¡æˆ‘å¯ä»¥è®©æ‚¨è‡ªå·±è¿›è¡Œæ“ä½œğŸ‘ğŸ¼ã€‚

    ![img](https://cy-1256894686.cos.ap-beijing.myqcloud.com/cy/2020-04-24-171645.gif)

    Fig. 1.8: Repeat previous steps for Input 2 & Input 3

    > Notes:
    > å› ä¸ºç‚¹ç§¯å¾—åˆ†å‡½æ•° `query`å’Œ`key`çš„ç»´åº¦å¿…é¡»å§‹ç»ˆç›¸åŒ.ä½†æ˜¯`value`çš„ç»´æ•°å¯èƒ½ä¸`query`å’Œ`key`çš„ç»´æ•°ä¸åŒã€‚å› æ­¤è¾“å‡ºç»“æœå°†éµå¾ª`value`çš„ç»´åº¦ã€‚

## ä»£ç 

è¿™é‡Œè¦ä¸€ä»½ pytorch ä»£ç ğŸ¤—,pytorch æ˜¯ä¸€ç§éå¸¸å—æ¬¢è¿çš„æ·±åº¦å­¦ä¹ æ¡†æ¶.ä¸ºäº†åœ¨ä»¥ä¸‹ä»£ç æ®µä¸­ä½¿ç”¨â€œ @â€è¿ç®—ç¬¦ï¼Œ`.T`å’Œ`None`ç´¢å¼•çš„APIï¼Œè¯·ç¡®ä¿æ‚¨ä½¿ç”¨çš„Pythonâ‰¥3.6å’ŒPyTorch 1.3.1ã€‚åªéœ€å°†å®ƒä»¬å¤åˆ¶å¹¶ç²˜è´´åˆ°Python / IPython REPLæˆ–Jupyter Notebookä¸­å³å¯ã€‚

**Step 1: å‡†å¤‡è¾“å…¥**

```
import torch

x = [
  [1, 0, 1, 0], # Input 1
  [0, 2, 0, 2], # Input 2
  [1, 1, 1, 1]  # Input 3
 ]
x = torch.tensor(x, dtype=torch.float32)
```

**Step 2: åˆå§‹åŒ–æƒé‡**

```
w_key = [
  [0, 0, 1],
  [1, 1, 0],
  [0, 1, 0],
  [1, 1, 0]
]
w_query = [
  [1, 0, 1],
  [1, 0, 0],
  [0, 0, 1],
  [0, 1, 1]
]
w_value = [
  [0, 2, 0],
  [0, 3, 0],
  [1, 0, 3],
  [1, 1, 0]
]
w_key = torch.tensor(w_key, dtype=torch.float32)
w_query = torch.tensor(w_query, dtype=torch.float32)
w_value = torch.tensor(w_value, dtype=torch.float32)
```

**Step 3:å¯¼å‡º`key`, `query` and `value`çš„è¡¨ç¤º**

```
keys = x @ w_key
querys = x @ w_query
values = x @ w_value

print(keys)
# tensor([[0., 1., 1.],
#         [4., 4., 0.],
#         [2., 3., 1.]])

print(querys)
# tensor([[1., 0., 2.],
#         [2., 2., 2.],
#         [2., 1., 3.]])

print(values)
# tensor([[1., 2., 3.],
#         [2., 8., 0.],
#         [2., 6., 3.]])
```

**Step 4: è®¡ç®—è¾“å…¥çš„æ³¨æ„åŠ›å¾—åˆ†(`attention scores`)**

```
attn_scores = querys @ keys.T

# tensor([[ 2.,  4.,  4.],  # attention scores from Query 1
#         [ 4., 16., 12.],  # attention scores from Query 2
#         [ 4., 12., 10.]]) # attention scores from Query 3
```

**Step 5: è®¡ç®—softmax**

```
from torch.nn.functional import softmax

attn_scores_softmax = softmax(attn_scores, dim=-1)
# tensor([[6.3379e-02, 4.6831e-01, 4.6831e-01],
#         [6.0337e-06, 9.8201e-01, 1.7986e-02],
#         [2.9539e-04, 8.8054e-01, 1.1917e-01]])

# For readability, approximate the above as follows
attn_scores_softmax = [
  [0.0, 0.5, 0.5],
  [0.0, 1.0, 0.0],
  [0.0, 0.9, 0.1]
]
attn_scores_softmax = torch.tensor(attn_scores_softmax)
```

**Step 6: å°†`attention scores`ä¹˜ä»¥`value`**

```
weighted_values = values[:,None] * attn_scores_softmax.T[:,:,None]

# tensor([[[0.0000, 0.0000, 0.0000],
#          [0.0000, 0.0000, 0.0000],
#          [0.0000, 0.0000, 0.0000]],
# 
#         [[1.0000, 4.0000, 0.0000],
#          [2.0000, 8.0000, 0.0000],
#          [1.8000, 7.2000, 0.0000]],
# 
#         [[1.0000, 3.0000, 1.5000],
#          [0.0000, 0.0000, 0.0000],
#          [0.2000, 0.6000, 0.3000]]])
```

**Step 7: å¯¹åŠ æƒåçš„`value`æ±‚å’Œä»¥å¾—åˆ°è¾“å‡º**

```
outputs = weighted_values.sum(dim=0)

# tensor([[2.0000, 7.0000, 1.5000],  # Output 1
#         [2.0000, 8.0000, 0.0000],  # Output 2
#         [2.0000, 7.8000, 0.3000]]) # Output 3
```

> ***Note\****
> PyTorch has provided an API for this called* `*nn.MultiheadAttention*`*. However, this API requires that you feed in key, query and value PyTorch tensors. Moreover, the outputs of this module undergo a linear transformation.*

**Step 8:å¯¹è¾“å…¥2é‡å¤æ­¥éª¤4â€“7**

## æ‰©å±•åˆ°Transformers

é‚£ä¹ˆæˆ‘ä»¬è¯¥ä½•å»ä½•ä»ï¼ŸTransformersï¼ç¡®å®ï¼Œæˆ‘ä»¬ç”Ÿæ´»åœ¨æ·±åº¦å­¦ä¹ ç ”ç©¶å’Œé«˜è®¡ç®—èµ„æºä»¤äººå…´å¥‹çš„æ—¶ä»£ã€‚Transformersæ˜¯ [Attention Is All You Need](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a#9abf)çš„åº”ç”¨ã€‚ç ”ç©¶äººå‘˜ä»è¿™é‡Œå¼€å§‹è¿›è¡Œç»„è£…ï¼Œåˆ‡å‰²ï¼Œæ·»åŠ å’Œæ‰©å±•é›¶ä»¶ï¼Œå¹¶å°†å…¶ç”¨é€”æ‰©å±•åˆ°æ›´å¤šçš„è¯­è¨€ä»»åŠ¡ã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘å°†ç®€è¦æåŠå¦‚ä½•å°†è‡ª`Self-Attention`æ‰©å±•åˆ°`Transformer`ä½“ç³»ç»“æ„ã€‚(ä¸“ä¸šæœ¯è¯­ä¸è¯‘)

Within the self-attention module:

- Dimension
- Bias

Inputs to the self-attention module:

- Embedding module
- Positional encoding
- Truncating
- Masking

Adding more self-attention modules:

- Multihead
- Layer stacking

Modules between self-attention modules:

- Linear transformations
- LayerNorm

------

## References

[Attention Is All You Need](https://arxiv.org/abs/1706.03762) (arxiv.org)

[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) (jalammar.github.io)

## Related Articles

[Attn: Illustrated Attention](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3) (towardsdatascience.com)

## Credits

Special thanks to Xin Jie, Serene, Ren Jie, Kevin and Wei Yih for ideas, suggestions and corrections to this article.

*Follow me on Twitter* [*@remykarem*](https://twitter.com/remykarem) *for digested articles and other tweets on AI, ML, Deep Learning and Python.*

